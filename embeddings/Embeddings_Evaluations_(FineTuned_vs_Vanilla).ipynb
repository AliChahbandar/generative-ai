{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "Mbyy88dG-Qq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Zachary Thorman](https://github.com/zthor5)|"
      ],
      "metadata": {
        "id": "ByHvc05m-SYH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtwZ8DZGJfUv"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook will be used to evaluate the performance of different combinations of finetuning, RAG, & models for evaluating their performance.\n",
        "\n",
        "This Notebook currently shows the capabilities using Gemini 1.5 Pro with different combinations of embedding models, utilizing ChromaDB to create a RAG architecture & evaluate.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1LizTwffekG1RfvpaRYAiuNTRM_Q8duVg\" width=\"70%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n",
        "In this section, you will install needed dependencies & define the Google Cloud project where you want to connect to Vertex AI."
      ],
      "metadata": {
        "id": "2t--gKa8HDZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "gM2it3BmSEo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JbXe7Oodc5dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2892bbd1-5ed7-4947-a032-a876e9b369d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.4/581.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet google-generativeai chromadb pymupdf google-cloud-storage langchain==0.1.20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwmKt115PxK8"
      },
      "source": [
        "Then import the modules you'll use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "muuhsDmmKdHi"
      },
      "outputs": [],
      "source": [
        "import textwrap, chromadb, random, re, time, datetime, json, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymupdf\n",
        "\n",
        "from vertexai.generative_models import GenerativeModel, Part\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "from google.cloud import storage\n",
        "\n",
        "from IPython.display import Markdown, HTML, display\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "# Import LangChain components\n",
        "import langchain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6tZGHUDOCFW"
      },
      "source": [
        "### Restart runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After its restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "ruVajAp-Htc4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ Wait for the kernel to finish restarting before you continue. ⚠️</b>\n",
        "</div>"
      ],
      "metadata": {
        "id": "GmqS2zdDHwki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment.\n",
        "\n",
        "This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
      ],
      "metadata": {
        "id": "ekYrnfAnH12N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "022W7-OUH5YL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Google Cloud project information, initialize Vertex AI, and add Secrets\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ],
      "metadata": {
        "id": "AALfV4LTH-xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizing Secrets to retrieve sensitive information\n",
        "# You can add your own projectID and location to run in your environment.\n",
        "\n",
        "PROJECT_ID = userdata.get('ProjectId') # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"    # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "YgDr8fuQIAbQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declaring Helper Class for Embedding's and LLM's"
      ],
      "metadata": {
        "id": "ccPd69qZJN2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "tWDl6Xb5ri6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clean_folders(PDF_Path):\n",
        "  # Create the directory if it doesn't exist\n",
        "  if not os.path.exists(PDF_Path):\n",
        "    os.makedirs(PDF_Path)\n",
        "  pdf_star = PDF_Path + \"*\"\n",
        "  !rm -rf {pdf_star}\n",
        "\n",
        "  if not os.path.exists(\"./output/\"):\n",
        "    os.makedirs(\"./output/\")\n",
        "  pdf_star = \"./output/\" + \"*\"\n",
        "  !rm -rf {pdf_star}\n",
        "\n",
        "def update_text(text = \"default text\"):\n",
        "    return HTML(\"\"\"\n",
        "        <p>{}</p>\n",
        "    \"\"\".format(text))\n",
        "\n",
        "def progress(value =1, max =1):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 60%'>\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "\n",
        "def download_bucket_to_local(bucket_uri, local_folder):\n",
        "  gcs_uri_list = []\n",
        "  storage_client = storage.Client()\n",
        "  bucket = storage_client.bucket(bucket_uri)\n",
        "  blobs = bucket.list_blobs()\n",
        "  for blob in blobs:\n",
        "    file_path = local_folder + blob.name\n",
        "    blob.download_to_filename(file_path)\n",
        "    gcs_uri_list.append(\"gs://\" + bucket_uri + \"/\" + blob.name)\n",
        "    print(f\"Downloaded: {blob.name}\")\n",
        "  return gcs_uri_list\n",
        "\n",
        "def update_text(text = \"default text\"):\n",
        "    return HTML(\"\"\"\n",
        "        <p>{}</p>\n",
        "    \"\"\".format(text))\n",
        "\n",
        "def progress(value =1, max =1):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 60%'>\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ],
      "metadata": {
        "id": "ePQK3JRtrgal"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class for Generative Model"
      ],
      "metadata": {
        "id": "WLWCPbyJVp0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenModel:\n",
        "    global generation_config\n",
        "    generation_config = {\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"temperature\": 0,\n",
        "    \"top_p\": 0.95}\n",
        "\n",
        "    def __init__(self, model_name = \"gemini-1.5-pro-001\", temperature = 1.0):\n",
        "        # Add a switch case to choose how to initialize the model <zthor>\n",
        "        self.model = GenerativeModel(model_name, generation_config = generation_config)\n",
        "\n",
        "        self.model_name = model_name # To be used later for picking models via a Match\n",
        "        self.temperature = temperature # To be used later for picking models via a Match\n",
        "\n",
        "    def get_answer(self, prompt):\n",
        "        match self.model_name:\n",
        "          case \"gemini-1.5-pro-001\":\n",
        "            print(\"Gemini!\")\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "          case \"gemma\":\n",
        "            print(\"Gemma!\")\n",
        "            return \"To be implemented\"\n",
        "          case _:\n",
        "            print(\"default fun!\")\n"
      ],
      "metadata": {
        "id": "cN3GBu0SJUJ6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tester = GenModel(\"gemini-1.5-pro-001\")\n",
        "Markdown(tester.get_answer(\"Use 5 words to answer this question: What is the meaning of life? \"))"
      ],
      "metadata": {
        "id": "cF7hnBjDpghS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "b1a4e7ae-2b59-4656-d25b-d31ef5c592bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To find your own meaning. \n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class for Plain Embedding Model"
      ],
      "metadata": {
        "id": "eOw74xojha_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vanilla_Embedding_Model(EmbeddingFunction):\n",
        "    def __call__(self, input: Documents) -> Embeddings: # https://github.com/chroma-core/chroma/issues/1496\n",
        "      embeddings = []\n",
        "      for doc in input:\n",
        "        vector = self.embed_model.get_embeddings([doc])\n",
        "        embeddings.append(vector[0].values)\n",
        "      return embeddings\n",
        "\n",
        "    def __init__(self, model_name= \"text-embedding-004\"):\n",
        "        self.embed_model = TextEmbeddingModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "hIjXaB10SmVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tester_embed = Vanilla_Embedding_Model(\"text-embedding-004\")\n",
        "print(tester_embed([\"To find purpose, meaning, and connection in our existence.\"]))"
      ],
      "metadata": {
        "id": "n-2-y_exqR5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aiko writing the Finetuning Embeddings Class\n",
        "- [Similar Notebook](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/tuned_text-embeddings.ipynb#scrollTo=kIJC46m7SXvW)\n",
        "- [GCP Guide for Vertex Embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings)\n",
        "\n",
        "- [This notebook does almost exactly what you need to do for this section](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/embeddings/intro_embeddings_tuning.ipynb)"
      ],
      "metadata": {
        "id": "ZPqL_X-L1b44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Do not worry about this format, You can change your code to work like this later\n",
        "# # Be initialized via a method, convert to embeddings via a method, and call the model endpoint via a method\n",
        "\n",
        "# class FineTuned_Embedding_Model:\n",
        "#     def __init__(self, model_name, local_folder_location_of_jsonl_and_tsv):\n",
        "#         self.embed_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "#         # Make sure to have JSONLs & TSV files for finetuning here\n",
        "#         # Create the finetuned model, using google credentials from earlier\n",
        "\n",
        "#     def get_embeddings(self, text) -> list:\n",
        "\n",
        "#         embedding = self.embed_model.get_embeddings([text])\n",
        "#         vector = embedding[0].values\n",
        "#         return vector\n",
        "#         # Change to utilize the Embeddings Endpoint\n",
        "#         # Will return a vector of embeddings\n",
        "\n",
        "#     def call_model_endpoint():\n",
        "#       # Call your Finetuned model from Vertex\n",
        "#       loader = DataFrameLoader(df, page_content_column=\"page_content\")\n"
      ],
      "metadata": {
        "id": "HL7gpg_l1boD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fegnGFpMS4AI"
      },
      "source": [
        "# ChromaDB Helper\n",
        "\n",
        "Key Point: Next, you will choose a model. Any embedding model will work for this tutorial, but for real applications it's important to choose a specific model and stick with it. The outputs of different models are not compatible with each other."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class chroma_db():\n",
        "  def __call__(self, input: Documents) -> Embeddings: # https://github.com/chroma-core/chroma/issues/1496\n",
        "    embeddings = []\n",
        "    for doc in input:\n",
        "      vector = self.embed_model.get_embeddings([doc])\n",
        "      embeddings.append(vector[0].values)\n",
        "    return embeddings\n",
        "\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.chunk_id = 0\n",
        "    self.client = chromadb.Client()\n",
        "\n",
        "  def create_collection(self, name, embedding_function, metadata):\n",
        "    self.collection_name = name\n",
        "    self.embedding_function = embedding_function\n",
        "    self.collection = self.client.create_collection(\n",
        "        name=name,\n",
        "        embedding_function=embedding_function,\n",
        "        metadata=metadata)\n",
        "\n",
        "  def add_pdfs(self, local_pdf_folder):\n",
        "    pdf_string_list = []\n",
        "    for pdf in os.listdir(local_pdf_folder):\n",
        "      pdf_text = \"\"\n",
        "      with pymupdf.open(local_pdf_folder + pdf) as doc:\n",
        "        for page in doc:\n",
        "          pdf_text += page.get_text()\n",
        "        pdf_string_list.append (pdf_text)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=10000,\n",
        "        chunk_overlap=200)\n",
        "\n",
        "    display_out = display(update_text(\"Start Chunking...\"), display_id=True)\n",
        "    for pdf_text in pdf_string_list:\n",
        "      chunks = text_splitter.split_text(pdf_text)\n",
        "      for chunk in chunks:\n",
        "        self.chunk_id+= 1\n",
        "        display_out.update(update_text(f\"Adding Chunk: {self.chunk_id}\"))\n",
        "        self.collection.add(documents= [chunk], ids=f\"chunk_{self.chunk_id}\")\n",
        "    display_out.update(update_text(f\"All Chunks loaded!\"))\n"
      ],
      "metadata": {
        "id": "ngIrZpz3iRCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Chroma & Embeddings"
      ],
      "metadata": {
        "id": "xf6eoyxNybY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING THE CODE\n",
        "\n",
        "# Only Store PDFs in the Bucket\n",
        "gcs_bucket = \"dmv-pdf-analysis\" # Do not put any slashes after uri!\n",
        "pdf_folder =\"./downloaded_pdfs/\" # Include a slash after the uri!\n",
        "\n",
        "create_clean_folders(pdf_folder)\n",
        "gcs_pdf_list = download_bucket_to_local(gcs_bucket, pdf_folder)\n"
      ],
      "metadata": {
        "id": "jExoGsl6rv8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma = chroma_db(\"My name is.. chroma DB client!\")\n",
        "\n",
        "chroma.create_collection(\"chroma_testing\", Vanilla_Embedding_Model(), {\"hnsw:space\": \"cosine\"})\n",
        "\n",
        "chroma.add_pdfs(pdf_folder)"
      ],
      "metadata": {
        "id": "sHBQECdrhCFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chroma.collection.get())\n",
        "print(chroma.collection.peek())\n",
        "print(chroma.collection.count())\n",
        "\n",
        "query_relevant_passage = chroma.collection.query(query_texts=[\"What do you have to do a a kid to get your license?\"], n_results=5)\n",
        "print(query_relevant_passage)\n",
        "print(query_relevant_passage['documents'][0][0])"
      ],
      "metadata": {
        "id": "vYRG03yht9Va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chroma.client.delete_collection(\"chroma_testing\")"
      ],
      "metadata": {
        "id": "Qgxi6jUEXGh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "n5Bb8Ni0q8bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill Sandwich choices for testing"
      ],
      "metadata": {
        "id": "pWEIF3hXq_0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThTbjAJ7eGP5"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "To learn more about how you can use the embeddings, check out the [examples](https://ai.google.dev/examples?keywords=embed) available. To learn how to use other services in the Gemini API, visit the [Python quickstart](https://ai.google.dev/gemini-api/docs/get-started/python)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2t--gKa8HDZ7",
        "U6tZGHUDOCFW",
        "ekYrnfAnH12N",
        "AALfV4LTH-xA",
        "tWDl6Xb5ri6z",
        "ZPqL_X-L1b44"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}